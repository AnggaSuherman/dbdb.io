{
    "Email": "prashasp@andrew.cmu.edu",

    
    "Name": "Apache Hive",
    
    
    "Description": "Apache Hive is a data warehouse software built on top of Hadoop that facilitates reading, writing and managing large datasets residing in distributed storage using SQL. Hive provides the necessary SQL abstraction so that SQL-like queries can be integrated with the underlying Java code without having to implement the queries in the low-level Java API. It allows structure to be projected onto data that is already in storage. Hive also provides a command line tool and a JDBC driver that users can use to connect to Hive.",
    "Description Citations": [
        "http://hive.apache.org/",
        "https://en.wikipedia.org/wiki/Apache_Hive"
    ],
    
    
    "History": "Apache Hive was co-created by Joydeep Sen Sarma and Ashish Thusoo during their stint at Facebook. Both of them realized that in order to make the best use of Hadoop, they would have to write some fairly complex Java Map-Reduce jobs. They realized that they would not be able to teach their fast growing engineering and analyst teams the skill set needed to be able to exploit Hadoop across the organization. SQL was the interface used widely by the engineers and analysts. While SQL could cater to most analytics requirements, the creators also wanted to bring in the programmability that Hadoop provides. Apache Hive was born out of these dual goals - an SQL-based declarative language that also allowed engineers to be able to plug in their own scripts and programs when SQL did not suffice. To facilitate the creation of data driven organizations, it was also built to store centralized metadata(Hadoop based) about all the datasets in the organization.",
    "History Citations": [
        "https://www.qubole.com/blog/big-data/founders-transformation-hadoop/?utm_source=Quora&utm_medium=Answer&utm_campaign=Gil&utm_content=History-of-Hive"
    ],
    
    
    "System Architecture Options": [],
    "System Architecture Description": "Apache Hive has 6 main components as part of its architecture.  1. __UI__ - The front-end where users can submit their queries.  2. __Driver__ - The component which receives queries, creates session handles and monitors the lifecycle and progress of the queries. The driver also collects the results of the query obtained after the Reduce operation.  3. __Compiler__ - The component that parses the query, does semantic analysis on the query (query expressions and other query blocks) and creates an execution plan for the same. The compiler creates the execution plan with the help of table metadata and partition metadata that it retrieves from the metastore. The execution plan is in the form of a series of map-reduce tasks. 4. __Optimizer__ - This component does more plan transformations on the execution plan generated by the Compiler to generate a more optimized execution plan. 5. __Metastore__ - The component that stores metadata about all the tables and the partitions in the warehouse. The data is stored in a traditional RDBMS format and caters to two important features of a data warehouse: data abstraction and data discovery. 6. __Execution Engine__ – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine executes the query plan in accordance with the dependency between the stages.",
    "System Architecture Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/Design#Design-Motivation",
        "https://en.wikipedia.org/wiki/Apache_Hive"
    ],
    
    
    "Query Interface Options": [
        "SQL"
    ],
    "Query Interface Description": "The query interface used by Hive is HiveQL. While it is based on SQL, HiveQL does not strictly follow the full SQL-92 standard.",
    "Query Interface Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/LanguageManual"
    ],
    
    
    "Data Model Options": [
        "Relational",
        "Other"
    ],
    "Data Model Description": "All the data is stored in HDFS as files. Within a particular database, the data in the tables is serialized and each table has its own corresponding HDFS directory. The data within the table can be sub-divided into partitions that can in turn determine how data is distributed within sub-directories under the table directory. The partition data can further be divided into buckets. ",
    "Data Model Citations": [
        "https://hortonworks.com/apache/hive/"
    ],
    
    
    "Storage Model Options": [
        "N-ary Storage Model",
        "Decomposition Storage Model",
        "Hybrid",
        "Custom"
    ],
    "Storage Model Description": "In Apache Hive, all the data is present in HDFS as files. The tables in Hive are similar to tables in Relational Databases. Databases contain tables, which are in turn made up of partitions. Apache Hive supports the following File Formats -  1. __Text File__ - Data is laid out in lines, with each line being a record. Lines are terminated by a newline character in the typical unix fashion.  2. __SequenceFile__ - They encode a key and a value for each record. The records are stored in a binary format and hence occupies lesser space than a text-based format would.  3. __Columnar File Formats__ (RCFile,  Parquet) - Allows column values to be stored adjacent to each other and provides all the advantages of DSM model.  4. __Avro Files__ - The format encodes the schema of its contents directly in the file which allows the user to store complex objects natively. Avro is not really a file format, it’s a file format plus a serialization and deserialization framework.  5. __ORC fIles__ - This is the Optimized Row Columnar file format that provides a highly efficient way to store Hive data and overcomes the limitations of other Hive file formats.  6. __Custom INPUTFORMAT and OUTPUTFORMAT__",
    "Storage Model Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/FileFormats",
        "https://blog.matthewrathbone.com/2016/09/01/a-beginners-guide-to-hadoop-storage-formats.html"
    ],
    
        
    "Storage Architecture Options": [
        "Disk-oriented",
        "In-Memory",
        "Hybrid"
    ],
    "Storage Architecture Description": "Hive has predominantly been a disk-based architecture since it was mainly used in data warehousing on multi-petabyte datasets spanning thousands of nodes. However, there are many interesting use cases with smaller datasets which require a more interactive setting. This called for a shift to in-memory architectures. Hive2 marks the beginning of Hive’s journey from a disk-centric architecture to a memory centric architecture through Hive LLAP ( Live Long and Process). Since memory costs are 100x times more than a disk, this shift has required a careful redesign of the system architecture to make best use of available resources while meeting the brief.",
    "Storage Architecture Citations": [
        "https://hortonworks.com/blog/apache-hive-going-memory-computing/"
    ],
    
    
    "Concurrency Control Options": [
        "Two-Phase Locking (Deadlock Prevention)"
    ],
    "Concurrency Control Description": "Apache Hive allows concurrent access through the use of locks. It supports shared lock and exclusive lock. In order to avoid deadlocks, all the objects to be locked are sorted in a lexicographic manner, and the locks are acquired in order.",
    "Concurrency Control Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/Locking",
        "https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"
    ],
    
    
    "Isolation Levels Options": [
        "Snapshot Isolation"
    ],
    "Isolation Levels Description": "Currently, Apache Hive only supports Snapshot Isolation. When the query begins, it is provided with a consistent snapshot of the database which it uses till the end of its execution. Other isolations may be added in the future depending on demand. With the addition of transactions in Hive 0.13, full ACID semantics is now provided at the row level, so that one application can add rows while another reads from the same partition without interfering with each other. Initially it was only at the partition level.",
    "Isolation Levels Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions"
    ],
    
    
    "Indexes Options": [
        "BitMap",
        "Other"
    ],
    "Indexes Description": "Hive supports Bitmap indexing and compact indexing. The Bitmap index is implemented as a byte-aligned bitmap compression.",
    "Indexes Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/IndexDev+Bitmap"
    ],
    
    
    "Foreign Keys Options": [
        "Not Supported"
    ],
    "Foreign Keys Description": "Hive does not support foreign keys. Hive is implemented as Schema-on-Read (data is applied to a schema/plan as it is read). Therefore, there is no need for referential integrity to be performed by Hive on the data. Instead integrity checks need to be explicitly enforced in the queries.",
    "Foreign Keys Citations": [
        "https://issues.apache.org/jira/browse/HIVE-13290"
    ],
    
    
    "Logging Options": [
        "Other"
    ],
    "Logging Description": "Hive uses log4j for logging. The default logging level is WARN for Hive releases prior to 0.13.0 and is INFO level for the later releases. These are mainly used for debugging purposes.",
    "Logging Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/GettingStarted#GettingStarted-HiveLogging"
    ],
    
    
    "Checkpoints Options": [
        "Non-Blocking"
    ],
    "Checkpoints Description": "Apache Hive is built atop Hadoop. The checkpointing mechanism is the same as in HDFS.",
    "Checkpoints Citations": [
        "http://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/"
    ],
    
    
    "Views Options": [
        "Virtual Views"
    ],
    "Views Description": "In Apache Hive, the view is purely a logical object and does not have any associated storage. Creating on a view follows the same process as executing any other query. Hive does not yet support updatable views and all views are read-only.",
    "Views Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-Create/Drop/AlterView"
    ],
    
    
    "Query Execution Options": [
        "Tuple-at-a-Time Model",
        "Vectorized Model"
    ],
    "Query Execution": "Earlier versions of Hive used the Tuple-at-a-Time execution model. Now, it also provides support for Vectorized query processing. Hive provides 2 execution engines each based on one query processing model. Users are free to choose which execution engine they want to use by changing a configuration parameter.",
    "Query Execution Citations": [
        "https://chatwithengineers.com/2016/08/29/a-survey-of-query-execution-engines-from-volcano-to-vectorized-processing/"
    ],
    
    
    "Stored Procedures Options": [
        "Supported"
    ],
    "Stored Procedures Description": "Hive Hybrid Procedural SQL On Hadoop (HPL/SQL) is an opensource tool that implements procedural SQL for Hive. It provides support for richer language in terms of allowing advanced expressions, various built-in functions and conditions to generate SQL on the fly based on the user configuration.",
    "Stored Procedures Citations": [
        "https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=59690156",
        "http://www.hplsql.org/why"
    ],

    
    "Joins Options": [
        "Hash",
        "Sort-Merge"
    ],
    "Joins Description": "Apache Hive supports Hybrid Grace Hash join(called map join in Hive). During the partitioning phase, an in-memory hash table is built for the first partition of R. Similarly, the first partition of S can do the probing directly against the in-memory partition of R. This enable the join of the first pair of partitions of R and S to be done at the end of the partitioning phase. Sort-Merge join is implemented as sort-merge bucket join in Hive. Each mapper reads a bucket from the first table and the corresponding bucket from the second table and then a merge sort join is performed.",
    "Joins Citations": [
        "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization",
        "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization#LanguageManualJoinOptimization-AutoConversiontoSMBMapJoin"
    ],
    
    
    "Query Compilation Options": [
        "Not Supported"
    ],
    "Query Compilation Description": "LLAP is the new hybrid execution model that is being shipped with HIVE 2.0 version and above, that makes queries much more efficient. It enables caching of columnar data, creates JIT-friendly operator pipelines, allows multi-threaded processing and pre-fetching to name a few. However, it is still in the early stages and we need to see how it matures.",
    "Query Compilation Citations": [
        "https://issues.apache.org/jira/browse/HIVE-7926"
    ],
    
    
    "Website": "http://hive.apache.org/",
    
    
    "Programming Language": [
        "Java"
    ],
    
    
    "Operating Systems": [
        "All OS with Java VM"
    ],
    
    
    "Project Type": [
        "Commercial"
    ],
    
    
    "Developer": "Apache Hive is an open source project run by volunteers at the Apache Software Foundation.",
    
    
    "Start Date": "2012",
    "Start Date Citations": [
        "https://hive.apache.org/downloads.html"
    ],
    
    
    "End Date": "Currently in Release 2.1.1",
    "End Date Citations": [
        "https://hive.apache.org/downloads.html"
    ],
    
    
    "Derived From": "Apache Hadoop",
    
    
    "License": [
        "Apache v2",
        "Open Source"
    ]
}